{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02277207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_and_add_text_values(file_path, output_file_path):\n",
    "    with open(file_path, 'r') as infile, open(output_file_path, 'w') as outfile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "        # Function 1: Remove lines starting with \"indicator_Order_\"\n",
    "        lines = [line for line in lines if not line.startswith('indicator_Order_')]\n",
    "\n",
    "        # Function 2: Add new indicator_Order attributes corresponding to the indicator_Text attribute \n",
    "        indicator_number = 1\n",
    "        for line in lines:\n",
    "            if line.startswith(\"indicator_Text_\"):\n",
    "                # Add a new line with the desired text\n",
    "                outfile.write(f'indicator_Order_{indicator_number}:{indicator_number}\\n{line}')\n",
    "                indicator_number += 1\n",
    "            elif line == \"\\n\":  # Check for a new section (empty line)\n",
    "                indicator_number = 1  # Reset the indicator number for a new section\n",
    "                outfile.write(line)\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "    # Call the new function to add quotes to PreUnit and PostUnit values\n",
    "    add_pre_and_postunit_quotes(output_file_path)\n",
    "\n",
    "def add_pre_and_postunit_quotes(input_file):\n",
    "    with open(input_file, 'r') as infile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "    with open(input_file, 'w') as outfile:\n",
    "        for line in lines:\n",
    "            if \"PreUnit:\" in line or \"PostUnit:\" in line:\n",
    "                # Find the index of the colon\n",
    "                colon_index = line.index(\":\")\n",
    "                # Extract the value after the colon\n",
    "                value = line[colon_index + 1:].strip()\n",
    "                # Put the value into quotation marks\n",
    "                line = line[:colon_index + 1] + f'\"{value}\"\\n'\n",
    "            outfile.write(line)\n",
    "\n",
    "# Specify your file paths\n",
    "file_path = \"/Users/Documents/Main/Postprocessing/Merged_models.txt\"\n",
    "output_file_path = \"/Users/Documents/Main/Postprocessing/Final_Merged_Model.txt\"\n",
    "\n",
    "# Example usage:\n",
    "remove_and_add_text_values(file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "176f25b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rearrange attributes\n",
    "\n",
    "class Section:\n",
    "    def __init__(self, lines):\n",
    "        self.lines = lines\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(sorted(self.lines)))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return sorted(self.lines) == sorted(other.lines)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_prefix(line, prefixes):\n",
    "        for prefix in prefixes:\n",
    "            line = line.replace(prefix, \"\")\n",
    "        return line.strip()\n",
    "\n",
    "    def process_lines(self, prefixes):\n",
    "        processed_lines = [self.remove_prefix(line, prefixes) for line in self.lines]\n",
    "        return processed_lines\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"/Users/noahritfeld/Documents/Main/Postprocessing/Final_Merged_Model.txt\"\n",
    "\n",
    "# Read the merged data from the specified file\n",
    "with open(file_path, \"r\") as file:\n",
    "    merged_data = file.read()\n",
    "\n",
    "# Split merged data into seperated fragments again\n",
    "separate_fragments = merged_data.strip().split(\"\\n\\n\")\n",
    "\n",
    "# Initialize sets to store unique sections for each category\n",
    "unique_topics = set()\n",
    "unique_indicators = set()\n",
    "unique_questions = set()\n",
    "\n",
    "# Define the prefixes to be removed\n",
    "prefixes = ['topic_', 'indicator_', 'question_']\n",
    "\n",
    "# Process each combined fragments\n",
    "for separate_fragment in separate_fragments:\n",
    "    # Split subsections of the combined fragments\n",
    "    subsections = separate_fragment.strip().split(\"\\n\")\n",
    "    # Process each separated fragment and create Section objects\n",
    "    topic, indicator, question = [], [], []\n",
    "    for subsection in subsections:\n",
    "        prefix, data = subsection.split(\":\", 1)\n",
    "        if prefix.startswith(\"topic_\"):\n",
    "            topic.append(f\"{prefix}:{data.strip()}\")\n",
    "        elif prefix.startswith(\"indicator_\"):\n",
    "            indicator.append(f\"{prefix}:{data.strip()}\")\n",
    "        elif prefix.startswith(\"question_\"):\n",
    "            question.append(f\"{prefix}:{data.strip()}\")\n",
    "    # Create Section objects and add them to respective sets\n",
    "    topic_section = Section(topic)\n",
    "    indicator_section = Section(indicator)\n",
    "    question_section = Section(question)\n",
    "\n",
    "    # Process lines and add to respective sets\n",
    "    unique_topics.add(Section(topic_section.process_lines(prefixes)))\n",
    "    unique_indicators.add(Section(indicator_section.process_lines(prefixes)))\n",
    "    unique_questions.add(Section(question_section.process_lines(prefixes)))\n",
    "\n",
    "# Open the same file in write mode to overwrite its content\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(\"Topics:\\n\")\n",
    "    for topic in unique_topics:\n",
    "        file.write(\"\\n\".join(topic.lines) + \"\\n\\n\")\n",
    "    file.write(\"Indicators:\\n\")\n",
    "    for indicator in unique_indicators:\n",
    "        file.write(\"\\n\".join(indicator.lines) + \"\\n\\n\")\n",
    "    file.write(\"Questions:\\n\")\n",
    "    for question in unique_questions:\n",
    "        file.write(\"\\n\".join(question.lines) + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a4191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified data written to '/Users/noahritfeld/Documents/Main/Postprocessing/Final_Merged_Model.txt'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_order_and_text_suffix(input_file_path):\n",
    "    # Read data from the input file\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Define a regular expression pattern to match and replace the Order keys and values\n",
    "    order_pattern = r'Order_(\\d+):'\n",
    "    modified_data_order = re.sub(order_pattern, r'Order:', data)\n",
    "\n",
    "    # Define a regular expression pattern to match and replace the Text keys and values\n",
    "    text_pattern = r'Text_(\\d+):'\n",
    "    modified_data_text = re.sub(text_pattern, r'Text:', modified_data_order)\n",
    "\n",
    "    # Open the same file in write mode to overwrite its content\n",
    "    with open(input_file_path, \"w\") as output_file:\n",
    "        output_file.write(modified_data_text)\n",
    "\n",
    "    print(f\"Modified data written to '{input_file_path}'\")\n",
    "\n",
    "# Example usage\n",
    "input_file_path = \"/Users/Documents/Main/Postprocessing/Final_Merged_Model.txt\"\n",
    "remove_order_and_text_suffix(input_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8481a4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged introduction and extra lines added to Final_Merged_Model.txt\n"
     ]
    }
   ],
   "source": [
    "def add_merged_introduction(input1_path, input2_path, output3_path):\n",
    "    # Read content from DSL_Model_A.txt\n",
    "    with open(input1_path, 'r') as input1_file:\n",
    "        input1_content = input1_file.read()\n",
    "\n",
    "    # Extract Name and Description from DSL_Model_A.txt\n",
    "    name_input1 = input1_content.split('\\n')[0].strip().split(':')[-1].strip(' \"')\n",
    "    description_input1 = input1_content.split('\\n')[3].strip().split(':')[-1].strip(' \"')\n",
    "\n",
    "    # Read content from DSL_Model_B.txt\n",
    "    with open(input2_path, 'r') as input2_file:\n",
    "        input2_content = input2_file.read()\n",
    "\n",
    "    # Extract Name and Description from DSL_Model_B.txt\n",
    "    name_input2 = input2_content.split('\\n')[0].strip().split(':')[-1].strip(' \"')\n",
    "    description_input2 = input2_content.split('\\n')[3].strip().split(':')[-1].strip(' \"')\n",
    "\n",
    "    # Combine Name and Description\n",
    "    merged_name = f'Name: \"Merged method of {name_input1} and {name_input2}\"'\n",
    "    merged_description = f'\"This is a description about {description_input1} and {description_input2}\"'\n",
    "\n",
    "    # Create the merged introduction section\n",
    "    merged_introduction = f\"{merged_name}\\nVersion:4.0\\nisPublic:true\\nDescription:{merged_description}\\nGPcreateNetwork:true\\nGPcreateMembers:true\\nOrigin:merged\\n\"\n",
    "\n",
    "    # Create extra lines for NameMethod1 and NameMethod2\n",
    "    extra_lines = f'NameMethod1: \"{name_input1}\"\\nNameMethod2: \"{name_input2}\"\\n\\n'\n",
    "\n",
    "    # Check if Final_Merged_Model.txt exists\n",
    "    try:\n",
    "        with open(output3_path, 'r') as existing_file:\n",
    "            existing_content = existing_file.read()\n",
    "    except FileNotFoundError:\n",
    "        existing_content = \"\"\n",
    "\n",
    "    # Open Final_Merged_Model.txt for writing\n",
    "    with open(output3_path, 'w') as output3_file:\n",
    "        # Write the merged introduction section to output3.txt\n",
    "        output3_file.write(merged_introduction)\n",
    "        output3_file.write(extra_lines)\n",
    "        output3_file.write(existing_content)\n",
    "\n",
    "    print(\"Merged introduction and extra lines added to Final_Merged_Model.txt\")\n",
    "\n",
    "# Specify the paths for input1.txt, input2.txt, and output3.txt\n",
    "input1_path = \"/Users/Documents/Main/Preprocessing/DSL_Model_A.txt\"\n",
    "input2_path = \"/Users/Documents/Main/Preprocessing/DSL_Model_B.txt\"\n",
    "output3_path = \"/Users/Documents/Main/Postprocessing/Final_Merged_Model.txt\"\n",
    "\n",
    "# Call the function\n",
    "add_merged_introduction(input1_path, input2_path, output3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e06cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
